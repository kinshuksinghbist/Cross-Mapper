{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# There is a more modular, clean and structured object oriented code in the other file attached.\n",
    "\n",
    "# This is an interactive version that takes you through my process, step by step. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading the model from drive\n",
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/file/d/1-5fOSHOSB9UXyP_enOoZNAMScrePVcMD/view'\n",
    "\n",
    "file_id = url.split('/d/')[1].split('/')[0]\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "gdown.download(download_url, output='best.pt', quiet=False)\n",
    "\n",
    "#Installing Dependencies\n",
    "# I prefer uv over pip, its much much faster than pip \n",
    "!pip install uv\n",
    "!uv pip install -r req.txt\n",
    "\n",
    "print(\"All packages installed!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the provided pre-trained model\n",
    "import os\n",
    "from ultralytics import YOLO\n",
    "import torch\n",
    "\n",
    "model_path = 'best.pt'\n",
    "\n",
    "print(f\"Loading model from: {model_path}\")\n",
    "\n",
    "#hardware acceleration\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\") # i used this for mac apple sillicon\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = YOLO(model_path)\n",
    "model.to(device)\n",
    "\n",
    "print(\"Model loaded successfully!\")\n",
    "\n",
    "\n",
    "broadcast_video_path = 'br-frame_matched.mp4'\n",
    "tacticam_video_path = 'ta-frame_matched.mp4'   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "#detecting and annotating players in the video\n",
    "def process_video(model, video_path, camera_name):\n",
    "    print(f\"\\nStarting detection for {camera_name} video: {video_path}\")\n",
    "\n",
    "    results_generator = model.predict(source=video_path, stream=True, conf=0.5, iou=0.7, classes=None, verbose=False)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video file {video_path}\")\n",
    "        return\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(f\"Video {camera_name} properties: {frame_width}x{frame_height} @ {fps} FPS, {total_frames} frames\")\n",
    "\n",
    "    output_video_path = f'results/Detections-{video_path}' \n",
    "\n",
    "    os.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    frame_count = 0\n",
    "    for result in results_generator:\n",
    "        frame_count += 1\n",
    "        if frame_count % 100 == 0:\n",
    "            print(f\"  {camera_name} processing frame {frame_count}/{total_frames}...\")\n",
    "\n",
    "        frame = result.orig_img\n",
    "        boxes = result.boxes\n",
    "\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = model.names[class_id]\n",
    "\n",
    "            color = (0, 255, 0) # Green bbox\n",
    "            if class_name == 'ball': \n",
    "                color = (0, 165, 255) # Orange for ball detections\n",
    "\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            label = f\"{class_name} {confidence:.2f}\"\n",
    "            cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Finished processing : {camera_name}  output saved to: {output_video_path}\")\n",
    "\n",
    "print(f\"Processing broadcast video: {broadcast_video_path}\")\n",
    "process_video(model, broadcast_video_path, \"Broadcast\")\n",
    "\n",
    "print(f\"Processing tacticam video: {tacticam_video_path}\")\n",
    "process_video(model, tacticam_video_path, \"Tacticam\")\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\nAll video processing tasks completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "\n",
    "#model names. We will use the first letter of every class in the label. Eg: 'G' for 'goalkeeper'\n",
    "class_names = model.names\n",
    "class_names = [x[0].upper() for y, x in class_names.items()]\n",
    "\n",
    "\n",
    "#ellipse marker for every detection, from the supervision library, makes them appealing\n",
    "ellipse_annotator = sv.EllipseAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    thickness=1\n",
    ")\n",
    "label_annotator = sv.LabelAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    text_color=sv.Color.from_hex('#000000'),\n",
    "    text_position=sv.Position.TOP_CENTER,\n",
    "    text_thickness=0\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "CUSTOM_TRACKER_YAML_PATH = 'custom_tracker-botsort-reid.yaml' \n",
    "\n",
    "#config settings for out custom BotSort Tracker \n",
    "custom_tracker_yaml_content = \"\"\"\n",
    "tracker_type: botsort\n",
    "track_high_thresh: 0.45 # threshold for the first association\n",
    "track_low_thresh: 0.15   # threshold for the second association\n",
    "new_track_thresh: 0.4   # threshold for init new track if the detection does not match any tracks\n",
    "track_buffer: 90 # Kept it a bit higher for long-term tracking\n",
    "match_thresh: 0.9 # threshold for matching tracks\n",
    "fuse_score: True  # Whether to fuse confidence scores with the iou distances before matching\n",
    "\n",
    "# BoT-SORT settings\n",
    "gmc_method: sparseOptFlow\n",
    "\n",
    "# ReID model related thresh\n",
    "proximity_thresh: 0.6 # minimum IoU for valid match with ReID\n",
    "appearance_thresh: 0.6 # minimum appearance similarity for ReID\n",
    "with_reid: True\n",
    "model: auto\n",
    "\"\"\"\n",
    "\n",
    "# Write the custom_tracker.yaml file\n",
    "with open(CUSTOM_TRACKER_YAML_PATH, 'w') as f:\n",
    "    f.write(custom_tracker_yaml_content)\n",
    "print(f\"Custom tracker config saved to: {CUSTOM_TRACKER_YAML_PATH}\")\n",
    "\n",
    "#Object Tracking\n",
    "def OBT(path, name):\n",
    "    output_video_path = f'results/BotSort-Tracked-{name}.mp4'\n",
    "\n",
    "    cap = cv2.VideoCapture(path)\n",
    "\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec\n",
    "\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "\n",
    "    print(f\"Writing output to: {output_video_path}\")\n",
    "\n",
    "    #tracking data stored per video.\n",
    "    from typing import Union, List, Dict, Tuple\n",
    "    \n",
    "    TrackingDataType = Dict[\n",
    "    int,  # frame number\n",
    "    Dict[\n",
    "        int,  # class_id\n",
    "        Dict[\n",
    "            int,  # tracking_id\n",
    "            List[Union[List[float], Tuple[float, float, float, float]]]  # bounding boxes\n",
    "            ]\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    tracked_data: TrackingDataType = {}\n",
    "\n",
    "    frame_number = 0\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            results = model.track(frame,\n",
    "            tracker= CUSTOM_TRACKER_YAML_PATH,\n",
    "            stream=True,\n",
    "            conf=0.52, \n",
    "            iou=0.7,   # IoU threshold for detection NMS\n",
    "            verbose=False,\n",
    "            persist=True)\n",
    "\n",
    "            frame_number+=1\n",
    "            if(frame_number % 25 == 0):\n",
    "                print(f\"Processed {frame_number} frames\")\n",
    "            for result in results:\n",
    "                framer = result.orig_img \n",
    "                tracked_data[frame_number] = {}\n",
    "                detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "                # if tracking id's are present in the results\n",
    "                if result.boxes.id is not None:\n",
    "                    all_detections = detections[detections.class_id != 0]\n",
    "\n",
    "                    #if tracker id is assigned\n",
    "                    if all_detections.tracker_id is not None:\n",
    "                        \n",
    "                        labels = [\n",
    "                            f\"#{int(tracker_id)} {class_names[int(class_id)]}\"\n",
    "                            for tracker_id, class_id in zip(all_detections.tracker_id, all_detections.class_id)\n",
    "                        ]\n",
    "\n",
    "                        for class_id, tracker_id, box in zip(\n",
    "                        all_detections.class_id,\n",
    "                        all_detections.tracker_id,\n",
    "                        all_detections.xyxy\n",
    "                        ):\n",
    "                            class_id = int(class_id)\n",
    "                            tracker_id = int(tracker_id)\n",
    "                            box = box.tolist()\n",
    "\n",
    "                            #adding the data in our data structure\n",
    "                            if class_id not in tracked_data[frame_number]:\n",
    "                                tracked_data[frame_number][class_id] = {}\n",
    "                            tracked_data[frame_number][class_id][tracker_id] = box\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        # when tracker_id is None\n",
    "                        labels = [\n",
    "                            \"NA\"\n",
    "                            for class_id in all_detections.class_id\n",
    "                        ]\n",
    "\n",
    "                    # annotation on the original frame\n",
    "                    annotated_framer = framer.copy()\n",
    "                    \n",
    "                    annotated_framer = ellipse_annotator.annotate(\n",
    "                        scene=annotated_framer,\n",
    "                        detections=all_detections\n",
    "                    )\n",
    "                    annotated_framer = label_annotator.annotate(\n",
    "                        scene=annotated_framer,\n",
    "                        detections=all_detections,\n",
    "                        labels=labels\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    # If no tracks are found ( at the very start)\n",
    "                    annotated_framer = framer.copy()\n",
    "                    \n",
    "                    all_detections = detections[detections.class_id != 0]\n",
    "                    all_detections = all_detections[all_detections.confidence > 0.51]\n",
    "                    annotated_framer = ellipse_annotator.annotate(\n",
    "                        scene=annotated_framer,\n",
    "                        detections=all_detections\n",
    "                    )\n",
    "                    labels_no_track = [\n",
    "                        \"NA\"\n",
    "                        for class_id in all_detections.class_id\n",
    "                    ]\n",
    "                    annotated_framer = label_annotator.annotate(\n",
    "                        scene=annotated_framer,\n",
    "                        detections=all_detections,\n",
    "                        labels=labels_no_track\n",
    "                    )\n",
    "\n",
    "                # Write to output video\n",
    "                out.write(annotated_framer)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    out.release()\n",
    "    \n",
    "    return tracked_data\n",
    "\n",
    "\n",
    "\n",
    "broadcast_tracked_players = OBT(broadcast_video_path,\"Broadcast\")\n",
    "tacticam_tracked_players = OBT(tacticam_video_path,\"Tacticam\")\n",
    "\n",
    "\n",
    "print(\"Video processing complete. Output saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#config\n",
    "MIN_MATCH_COUNT = 10  # number of good matches required to find a homography\n",
    "SIFT_RATIO_TEST_THRESHOLD = 0.75 # (0.7-0.8 is standard)\n",
    "RANSAC_REPROJECTION_THRESHOLD = 5.0 # maximum pixel distance for a point to be an inlier\n",
    "MIN_IOU_THRESHOLD = 0.1\n",
    "\n",
    "\n",
    "video1_path = broadcast_video_path\n",
    "video2_path = tacticam_video_path\n",
    "\n",
    "sift = cv2.SIFT_create()\n",
    "\n",
    "# FLANN parameters\n",
    "FLANN_INDEX_KDTREE = 1\n",
    "index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "search_params = dict(checks=50)\n",
    "flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "#data structure for mapped objects\n",
    "consistent_object_mapping = {}\n",
    "\n",
    "def get_frame(video_path, frame_number):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    if not ret:\n",
    "        print(f\"Warning: Could not read frame {frame_number} from {video_path}\")\n",
    "        return None\n",
    "    return frame\n",
    "\n",
    "#transforms a single bounding box using homography \n",
    "def transform_bbox(bbox, H):\n",
    "\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    corners = np.array([\n",
    "        [x1, y1],\n",
    "        [x2, y1],\n",
    "        [x2, y2],\n",
    "        [x1, y2]\n",
    "    ], dtype=np.float32).reshape(-1, 1, 2) # reshape for perspectiveTransform\n",
    "\n",
    "    # apply the homography\n",
    "    transformed_corners = cv2.perspectiveTransform(corners, H)\n",
    "\n",
    "    # calculating the new bounding box from the transformed corners\n",
    "    x_new = transformed_corners[:, 0, 0]\n",
    "    y_new = transformed_corners[:, 0, 1]\n",
    "\n",
    "    new_x1 = float(min(x_new))\n",
    "    new_y1 = float(min(y_new))\n",
    "    new_x2 = float(max(x_new))\n",
    "    new_y2 = float(max(y_new))\n",
    "\n",
    "    return [new_x1, new_y1, new_x2, new_y2]\n",
    "\n",
    "def calculate_iou(boxA, boxB):\n",
    "\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3]) \n",
    "    \n",
    "\n",
    "    # compute the area of intersection rectangle\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    # compute the area of both the prediction and ground-truth rectangles\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "    # edge case where boxes have zero area\n",
    "    if boxAArea <= 0 or boxBArea <= 0:\n",
    "        return 0.0\n",
    "\n",
    "    # compute the intersection over union\n",
    "    unionArea = float(boxAArea + boxBArea - interArea)\n",
    "    if unionArea <= 0:\n",
    "        return 0.0\n",
    "    \n",
    "    iou = interArea / unionArea\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "frame_numbers_v1 = list(broadcast_tracked_players.keys()) if broadcast_tracked_players else [0]\n",
    "frame_numbers_v2 = list(tacticam_tracked_players.keys()) if tacticam_tracked_players else [0]\n",
    "max_frames = max(max(frame_numbers_v1), max(frame_numbers_v2)) + 1\n",
    "\n",
    "for frame_num in range(max_frames):\n",
    "    print(f\"\\nProcessing Frame {frame_num}...\")\n",
    "\n",
    "    # Skip if no tracking data for this frame in both results\n",
    "    if frame_num not in broadcast_tracked_players and frame_num not in tacticam_tracked_players:\n",
    "        print(f\"No tracking data for frame {frame_num}, skipping.\")\n",
    "        consistent_object_mapping[frame_num] = {}\n",
    "        continue\n",
    "\n",
    "    frame1 = get_frame(video1_path, frame_num)\n",
    "    frame2 = get_frame(video2_path, frame_num)\n",
    "\n",
    "    if frame1 is None or frame2 is None:\n",
    "        print(f\"Skipping frame {frame_num}\")\n",
    "        consistent_object_mapping[frame_num] = {}\n",
    "        continue\n",
    "\n",
    "    # Convert frames to grayscale for SIFT\n",
    "    gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "    gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    #SIFT Feature Detection and Description\n",
    "    kp1, des1 = sift.detectAndCompute(gray1, None)\n",
    "    kp2, des2 = sift.detectAndCompute(gray2, None)\n",
    "\n",
    "    if des1 is None or des2 is None or len(kp1) < MIN_MATCH_COUNT or len(kp2) < MIN_MATCH_COUNT:\n",
    "        print(f\"Not enough SIFT features detected in frame {frame_num} for homography calculation.\")\n",
    "        consistent_object_mapping[frame_num] = {} # Store an empty entry\n",
    "        continue\n",
    "\n",
    "    # FLANN-based feature matching with ratio test\n",
    "    try:\n",
    "        matches = flann.knnMatch(des1, des2, k=2)\n",
    "    except cv2.error as e:\n",
    "        print(f\"Error during feature matching in frame {frame_num}: {e}\")\n",
    "        consistent_object_mapping[frame_num] = {}\n",
    "        continue\n",
    "\n",
    "    #Lowe's ratio test to filter good matches\n",
    "    good_matches = []\n",
    "    for match_pair in matches:\n",
    "        if len(match_pair) == 2:  # ensuring we have 2 matches\n",
    "            m, n = match_pair\n",
    "            if m.distance < SIFT_RATIO_TEST_THRESHOLD * n.distance:\n",
    "                good_matches.append(m)\n",
    "\n",
    "    if len(good_matches) < MIN_MATCH_COUNT:\n",
    "        print(f\"Not enough good SIFT matches ({len(good_matches)}) in frame {frame_num} for homography calculation.\")\n",
    "        consistent_object_mapping[frame_num] = {} # Store an empty entry\n",
    "        continue\n",
    "\n",
    "    src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "    # calculating homography matrix H using ransac\n",
    "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, RANSAC_REPROJECTION_THRESHOLD)\n",
    "\n",
    "    if H is None:\n",
    "        print(f\"Could not calculate homography for frame {frame_num}.\")\n",
    "        consistent_object_mapping[frame_num] = {} # Store an empty entry\n",
    "        continue\n",
    "\n",
    "    print(f\"Homography calculated for frame {frame_num} with {len(good_matches)} matches\")\n",
    "\n",
    "    current_frame_mapping = {}\n",
    "    \n",
    "    v1_frame_data = broadcast_tracked_players.get(frame_num, {})\n",
    "    v2_frame_data = tacticam_tracked_players.get(frame_num, {})\n",
    "\n",
    "    for class_id_v1, tracking_ids_v1 in v1_frame_data.items():\n",
    "        if class_id_v1 not in current_frame_mapping:\n",
    "            current_frame_mapping[class_id_v1] = {}\n",
    "            \n",
    "        for tracking_id_v1, bbox_data_v1 in tracking_ids_v1.items():\n",
    "            if isinstance(bbox_data_v1, list):\n",
    "                if len(bbox_data_v1) == 4 and all(isinstance(x, (int, float)) for x in bbox_data_v1):\n",
    "                    # bbox format [x1,y1,x2,y2]\n",
    "                    bbox_v1 = bbox_data_v1\n",
    "                else:\n",
    "                    print(f\"Invalid bbox format for tracking_id {tracking_id_v1}\")\n",
    "                    continue\n",
    "            else:\n",
    "                print(f\"error for tracking_id {tracking_id_v1}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                transformed_bbox_v2 = transform_bbox(bbox_v1, H)\n",
    "            except Exception as e:\n",
    "                print(f\"Error transforming bbox: {e}\")\n",
    "                continue\n",
    "\n",
    "            best_iou = MIN_IOU_THRESHOLD\n",
    "            best_match_info = None\n",
    "            \n",
    "            for class_id_v2, tracking_ids_v2 in v2_frame_data.items():\n",
    "                #matching only same class_ids \n",
    "                if class_id_v1 != class_id_v2:\n",
    "                    continue\n",
    "\n",
    "                for tracking_id_v2, bbox_data_v2 in tracking_ids_v2.items():\n",
    "                    # Handle different bbox formats for video 2\n",
    "                    if isinstance(bbox_data_v2, list):\n",
    "                        if len(bbox_data_v2) == 4 and all(isinstance(x, (int, float)) for x in bbox_data_v2):\n",
    "                            bbox_v2_detected = bbox_data_v2\n",
    "                        else:\n",
    "                            continue\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                    iou = calculate_iou(transformed_bbox_v2, bbox_v2_detected)\n",
    "\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_match_info = {\n",
    "                            'class_id': class_id_v2,\n",
    "                            'tracking_id': tracking_id_v2,\n",
    "                            'bbox': bbox_v2_detected,\n",
    "                            'iou': iou\n",
    "                        }\n",
    "            \n",
    "            # updating the data structure\n",
    "            object_mapping_entry = {\n",
    "                'video1_bbox': bbox_v1,\n",
    "                'video2_transformed_bbox': transformed_bbox_v2,\n",
    "                'video2_matched_detection': best_match_info # will be None if no good match\n",
    "            }\n",
    "\n",
    "            current_frame_mapping[class_id_v1][tracking_id_v1] = object_mapping_entry\n",
    "\n",
    "    consistent_object_mapping[frame_num] = current_frame_mapping\n",
    "\n",
    "print(f\"Cross Mapping Successful!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TRACKER ID MAPPINGS (Broadcast  -> Tacticam)\")\n",
    "\n",
    "for frame_num in sorted(consistent_object_mapping.keys()):\n",
    "    frame_data = consistent_object_mapping[frame_num]\n",
    "    if not frame_data:\n",
    "        continue\n",
    "    \n",
    "    has_matches = False\n",
    "    for class_id, tracks in frame_data.items():\n",
    "        for track_id, mapping in tracks.items():\n",
    "            if mapping.get('video2_matched_detection') is not None:\n",
    "                has_matches = True\n",
    "                break\n",
    "        if has_matches:\n",
    "            break\n",
    "    \n",
    "    if has_matches:\n",
    "        print(f\"\\nFrame {frame_num}:\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        for class_id, tracks in frame_data.items():\n",
    "            class_has_matches = False\n",
    "            for track_id, mapping in tracks.items():\n",
    "                if mapping.get('video2_matched_detection') is not None:\n",
    "                    class_has_matches = True\n",
    "                    break\n",
    "            \n",
    "            if class_has_matches:\n",
    "                print(f\"  Class {class_id} (e.g., {'Player' if class_id == 0 else 'Ball' if class_id == 1 else 'Unknown'}):\")\n",
    "                \n",
    "                for track_id_v1, mapping in tracks.items():\n",
    "                    match_info = mapping.get('video2_matched_detection')\n",
    "                    if match_info is not None:\n",
    "                        track_id_v2 = match_info['tracking_id']\n",
    "                        iou_score = match_info['iou']\n",
    "                        print(f\"    Broadcast Track {track_id_v1} â†’ Tacticam Track {track_id_v2} (IoU: {iou_score:.3f})\")\n",
    "\n",
    "\n",
    "tracker_mapping_counts = {}  # {(class_id, track_id_v1, track_id_v2): count}\n",
    "for frame_num, frame_data in consistent_object_mapping.items():\n",
    "    for class_id, tracks in frame_data.items():\n",
    "        for track_id_v1, mapping in tracks.items():\n",
    "            match_info = mapping.get('video2_matched_detection')\n",
    "            if match_info is not None:\n",
    "                track_id_v2 = match_info['tracking_id']\n",
    "                key = (class_id, track_id_v1, track_id_v2)\n",
    "                tracker_mapping_counts[key] = tracker_mapping_counts.get(key, 0) + 1\n",
    "\n",
    "sorted_mappings = sorted(tracker_mapping_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "simplified_mappings = {}\n",
    "for frame_num, frame_data in consistent_object_mapping.items():\n",
    "    simplified_mappings[frame_num] = {}\n",
    "    for class_id, tracks in frame_data.items():\n",
    "        for track_id_v1, mapping in tracks.items():\n",
    "            match_info = mapping.get('video2_matched_detection')\n",
    "            if match_info is not None:\n",
    "                if class_id not in simplified_mappings[frame_num]:\n",
    "                    simplified_mappings[frame_num][class_id] = {}\n",
    "                simplified_mappings[frame_num][class_id][track_id_v1] = {\n",
    "                    'tacticam_id': match_info['tracking_id'],\n",
    "                    'iou': match_info['iou']\n",
    "                }\n",
    "\n",
    "import json\n",
    "with open('tracker_id_mappings.json', 'w') as f:\n",
    "    json.dump(simplified_mappings, f, indent=2)\n",
    "print(\"\\nTracker mappings saved to 'tracker_id_mappings.json'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import json\n",
    "import supervision as sv\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "# To avoid ID flickering, we create one master mapping based on the most frequent\n",
    "# associations found across all frames. We use the 'sorted_mappings' variable\n",
    "# that was created in the previous cell.\n",
    "\n",
    "# This dictionary will map a tacticam_id to its most likely broadcast_id.\n",
    "tacticam_to_broadcast_id_map = {}\n",
    "\n",
    "broadcast_ids_mapped = set()\n",
    "tacticam_ids_mapped = set()\n",
    "\n",
    "ellipse_annotator_final = sv.EllipseAnnotator(\n",
    "    color=sv.ColorPalette.from_hex([\"#000000\", \"#000000\", \"#2E2704\"]),\n",
    "    thickness=1\n",
    ")\n",
    "label_annotator_final = sv.LabelAnnotator(\n",
    "    color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "    text_color=sv.Color.from_hex('#000000'),\n",
    "    text_position=sv.Position.TOP_CENTER,\n",
    "    text_thickness=0\n",
    ")\n",
    "\n",
    "for (class_id, b_id, t_id), count in sorted_mappings:\n",
    "    if b_id not in broadcast_ids_mapped and t_id not in tacticam_ids_mapped:\n",
    "        tacticam_to_broadcast_id_map[t_id] = b_id\n",
    "        broadcast_ids_mapped.add(b_id)\n",
    "        tacticam_ids_mapped.add(t_id)\n",
    "\n",
    "print(\"Created a stable global mapping for consistent IDs.\")\n",
    "print(f\"Total stable mappings created: {len(tacticam_to_broadcast_id_map)}\")\n",
    "\n",
    "def annotate_with_consistent_ids(input_video_path: str,\n",
    "                                 output_video_name: str,\n",
    "                                 tracked_data: dict,\n",
    "                                 is_broadcast_video: bool):\n",
    "    \n",
    "    output_path = f\"results/{output_video_name}\"\n",
    "    print(f\"\\n Starting consistent annotation for: {input_video_path}\")\n",
    "\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "    frame_num = 0\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        frame_num += 1\n",
    "        \n",
    "        detections_in_frame = tracked_data.get(frame_num, {})\n",
    "        \n",
    "        if not detections_in_frame:\n",
    "            out.write(frame) # original frame if no detections\n",
    "            continue\n",
    "\n",
    "        all_bboxes = []\n",
    "        all_class_ids = []\n",
    "        all_tracker_ids = []\n",
    "\n",
    "        for class_id, tracks in detections_in_frame.items():\n",
    "            for tracker_id, bbox in tracks.items():\n",
    "                all_bboxes.append(bbox)\n",
    "                all_class_ids.append(class_id)\n",
    "                all_tracker_ids.append(tracker_id)\n",
    "\n",
    "        # Convert to supervision Detections object\n",
    "        sv_detections = sv.Detections(\n",
    "            xyxy=np.array(all_bboxes),\n",
    "            class_id=np.array(all_class_ids),\n",
    "            tracker_id=np.array(all_tracker_ids)\n",
    "        )\n",
    "\n",
    "        # Generate consistent labels\n",
    "        labels = []\n",
    "        for tracker_id, class_id in zip(sv_detections.tracker_id, sv_detections.class_id):\n",
    "            class_name = model.names[class_id][0].upper()\n",
    "            \n",
    "            if is_broadcast_video:\n",
    "                #  broadcast ID is the master ID\n",
    "                final_id = tracker_id\n",
    "            else: \n",
    "                #its corresponding master (broadcast) ID\n",
    "                final_id = tacticam_to_broadcast_id_map.get(tracker_id, f\"T{tracker_id}\")\n",
    "            \n",
    "            labels.append(f\"#{final_id} {class_name}\")\n",
    "\n",
    "        # Annotate the frame\n",
    "        annotated_frame = frame.copy()\n",
    "        annotated_frame = ellipse_annotator_final.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=sv_detections\n",
    "        )\n",
    "        annotated_frame = label_annotator_final.annotate(\n",
    "            scene=annotated_frame,\n",
    "            detections=sv_detections,\n",
    "            labels=labels\n",
    "        )\n",
    "        \n",
    "        out.write(annotated_frame)\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    print(f\"Consistently annotated video to: {output_path}\")\n",
    "\n",
    "\n",
    "annotate_with_consistent_ids(\n",
    "    input_video_path=broadcast_video_path,\n",
    "    output_video_name=\"Final-Consistent-Broadcast.mp4\",\n",
    "    tracked_data=broadcast_tracked_players,\n",
    "    is_broadcast_video=True\n",
    ")\n",
    "\n",
    "annotate_with_consistent_ids(\n",
    "    input_video_path=tacticam_video_path,\n",
    "    output_video_name=\"Final-Consistent-Tacticam.mp4\",\n",
    "    tracked_data=tacticam_tracked_players,\n",
    "    is_broadcast_video=False\n",
    ")\n",
    "\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\n All videos have been generated again with consistent tracking IDs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "broadcast_cap = cv2.VideoCapture(\"results/Final-Consistent-Broadcast.mp4\")\n",
    "tacticam_cap = cv2.VideoCapture(\"results/Final-Consistent-Tacticam.mp4\")\n",
    "\n",
    "width = int(broadcast_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(broadcast_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = broadcast_cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "# output width will be double the width \n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(\"results/Final_Result.mp4\", fourcc, fps, (width * 2, height))\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret1, frame1 = broadcast_cap.read()\n",
    "    ret2, frame2 = tacticam_cap.read()\n",
    "\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "    combined_frame = np.hstack((frame1, frame2))\n",
    "    out.write(combined_frame)\n",
    "\n",
    "broadcast_cap.release()\n",
    "tacticam_cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "print(\"Your videos have been consistently mapped. Your Output is Ready.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb00bb9e",
   "metadata": {},
   "source": [
    "# This is the Object Oriented Clean version of the Cross Mapper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2c6d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "\n",
    "import gdown\n",
    "\n",
    "url = 'https://drive.google.com/file/d/1-5fOSHOSB9UXyP_enOoZNAMScrePVcMD/view'\n",
    "\n",
    "file_id = url.split('/d/')[1].split('/')[0]\n",
    "download_url = f'https://drive.google.com/uc?id={file_id}'\n",
    "\n",
    "gdown.download(download_url, output='best.pt', quiet=False)\n",
    "\n",
    "!pip install uv\n",
    "!uv pip install -r req.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79afd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import supervision as sv\n",
    "from ultralytics import YOLO\n",
    "from collections import defaultdict\n",
    "import json\n",
    "from typing import Union, List, Dict, Tuple\n",
    "\n",
    "TrackingDataType = Dict[\n",
    "    int,  \n",
    "    Dict[\n",
    "        int,  \n",
    "        Dict[\n",
    "            int,  \n",
    "            List[Union[List[float], Tuple[float, float, float, float]]]  # bounding boxes\n",
    "        ]\n",
    "    ]\n",
    "]\n",
    "\n",
    "class ObjectDetector:\n",
    "    def __init__(self, model_path='best.pt'):\n",
    "        self.model_path = model_path\n",
    "        self.model = None\n",
    "        self.device = self._get_device()\n",
    "        self._load_model()\n",
    "\n",
    "    def _get_device(self):\n",
    "        if torch.backends.mps.is_available():\n",
    "            return torch.device(\"mps\") \n",
    "        elif torch.cuda.is_available():\n",
    "            return torch.device(\"cuda\")\n",
    "        else:\n",
    "            return torch.device(\"cpu\")\n",
    "\n",
    "    def _load_model(self):\n",
    "        print(f\"Loading model from: {self.model_path}\")\n",
    "        self.model = YOLO(self.model_path)\n",
    "        self.model.to(self.device)\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(\"Model loaded successfully!\")\n",
    "\n",
    "    def detect_and_annotate(self, video_path: str, camera_name: str) -> None:\n",
    "        print(f\"\\nStarting detection for {camera_name} video: {video_path}\")\n",
    "\n",
    "        results_generator = self.model.predict(source=video_path, stream=True, conf=0.5, iou=0.7, classes=None, verbose=False)\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        if not cap.isOpened():\n",
    "            print(f\"Error: Could not open video file {video_path}\")\n",
    "            return\n",
    "\n",
    "        frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        print(f\"Video {camera_name} properties: {frame_width}x{frame_height} @ {fps} FPS, {total_frames} frames\")\n",
    "\n",
    "        output_video_path = f'results/Detections-{os.path.basename(video_path)}'\n",
    "        os.makedirs(os.path.dirname(output_video_path), exist_ok=True)\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "        frame_count = 0\n",
    "        for result in results_generator:\n",
    "            frame_count += 1\n",
    "            if frame_count % 100 == 0:\n",
    "                print(f\"  {camera_name} processing frame {frame_count}/{total_frames}...\")\n",
    "\n",
    "            frame = result.orig_img\n",
    "            boxes = result.boxes\n",
    "\n",
    "            for box in boxes:\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                confidence = float(box.conf[0])\n",
    "                class_id = int(box.cls[0])\n",
    "                class_name = self.model.names[class_id]\n",
    "\n",
    "                color = (0, 255, 0) \n",
    "                if class_name == 'ball':\n",
    "                    color = (0, 165, 255) \n",
    "\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                label = f\"{class_name} {confidence:.2f}\"\n",
    "                cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "            out.write(frame)\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Finished processing : {camera_name} . output saved to: {output_video_path}\")\n",
    "\n",
    "class ObjectTracker:\n",
    "    def __init__(self, model, tracker_config_path='custom_tracker-botsort-reid.yaml'):\n",
    "        self.model = model\n",
    "        self.tracker_config_path = tracker_config_path\n",
    "        self._write_tracker_config()\n",
    "\n",
    "        self.class_names = [x[0].upper() for y, x in model.names.items()]\n",
    "        self.ellipse_annotator = sv.EllipseAnnotator(\n",
    "            color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "            thickness=1\n",
    "        )\n",
    "        self.label_annotator = sv.LabelAnnotator(\n",
    "            color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "            text_color=sv.Color.from_hex('#000000'),\n",
    "            text_position=sv.Position.TOP_CENTER,\n",
    "            text_thickness=0\n",
    "        )\n",
    "\n",
    "    def _write_tracker_config(self):\n",
    "        custom_tracker_yaml_content = \"\"\"\n",
    "tracker_type: botsort\n",
    "track_high_thresh: 0.45 # threshold for the first association\n",
    "track_low_thresh: 0.15   # threshold for the second association\n",
    "new_track_thresh: 0.4   # threshold for init new track if the detection does not match any tracks\n",
    "track_buffer: 90 # Kept it a bit higher for long-term tracking\n",
    "match_thresh: 0.9 # threshold for matching tracks\n",
    "fuse_score: True  # Whether to fuse confidence scores with the iou distances before matching\n",
    "\n",
    "# BoT-SORT settings\n",
    "gmc_method: sparseOptFlow\n",
    "\n",
    "# ReID model related thresh\n",
    "proximity_thresh: 0.6 # minimum IoU for valid match with ReID\n",
    "appearance_thresh: 0.6 # minimum appearance similarity for ReID\n",
    "with_reid: True\n",
    "model: auto\n",
    "\"\"\"\n",
    "        with open(self.tracker_config_path, 'w') as f:\n",
    "            f.write(custom_tracker_yaml_content)\n",
    "        print(f\"Custom tracker config saved to: {self.tracker_config_path}\")\n",
    "\n",
    "    def track_and_annotate(self, video_path: str, camera_name: str) -> TrackingDataType:\n",
    "        output_video_path = f'results/BotSort-Tracked-{camera_name}.mp4'\n",
    "\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v') \n",
    "\n",
    "        out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
    "        print(f\"Writing output to: {output_video_path}\")\n",
    "\n",
    "        tracked_data: TrackingDataType = {}\n",
    "        frame_number = 0\n",
    "\n",
    "        while cap.isOpened():\n",
    "            success, frame = cap.read()\n",
    "            if success:\n",
    "                results = self.model.track(frame,\n",
    "                                           tracker=self.tracker_config_path,\n",
    "                                           stream=True,\n",
    "                                           conf=0.52,\n",
    "                                           iou=0.7,   # IoU threshold for detection NMS\n",
    "                                           verbose=False,\n",
    "                                           persist=True)\n",
    "\n",
    "                frame_number += 1\n",
    "                if frame_number % 25 == 0:\n",
    "                    print(f\"Processed {frame_number} frames\")\n",
    "\n",
    "                for result in results:\n",
    "                    framer = result.orig_img\n",
    "                    tracked_data[frame_number] = {}\n",
    "                    detections = sv.Detections.from_ultralytics(result)\n",
    "\n",
    "                    if result.boxes.id is not None:\n",
    "                        all_detections = detections[detections.class_id != 0]\n",
    "\n",
    "                        #if tracker id is assigned\n",
    "                        if all_detections.tracker_id is not None:\n",
    "                            labels = [\n",
    "                                f\"#{int(tracker_id)} {self.class_names[int(class_id)]}\"\n",
    "                                for tracker_id, class_id in zip(all_detections.tracker_id, all_detections.class_id)\n",
    "                            ]\n",
    "\n",
    "                            for class_id, tracker_id, box in zip(\n",
    "                                all_detections.class_id,\n",
    "                                all_detections.tracker_id,\n",
    "                                all_detections.xyxy\n",
    "                            ):\n",
    "                                class_id = int(class_id)\n",
    "                                tracker_id = int(tracker_id)\n",
    "                                box = box.tolist()\n",
    "\n",
    "                                #adding the data in our data structure\n",
    "                                if class_id not in tracked_data[frame_number]:\n",
    "                                    tracked_data[frame_number][class_id] = {}\n",
    "                                tracked_data[frame_number][class_id][tracker_id] = box\n",
    "                        else:\n",
    "                            # when tracker_id is None\n",
    "                            labels = [\"NA\" for class_id in all_detections.class_id]\n",
    "\n",
    "                        # annotation on the original frame\n",
    "                        annotated_framer = framer.copy()\n",
    "                        annotated_framer = self.ellipse_annotator.annotate(\n",
    "                            scene=annotated_framer,\n",
    "                            detections=all_detections\n",
    "                        )\n",
    "                        annotated_framer = self.label_annotator.annotate(\n",
    "                            scene=annotated_framer,\n",
    "                            detections=all_detections,\n",
    "                            labels=labels\n",
    "                        )\n",
    "                    else:\n",
    "                        # If no tracks are found ( at the very start)\n",
    "                        annotated_framer = framer.copy()\n",
    "                        all_detections = detections[detections.class_id != 0]\n",
    "                        all_detections = all_detections[all_detections.confidence > 0.51]\n",
    "                        annotated_framer = self.ellipse_annotator.annotate(\n",
    "                            scene=annotated_framer,\n",
    "                            detections=all_detections\n",
    "                        )\n",
    "                        labels_no_track = [\"NA\" for class_id in all_detections.class_id]\n",
    "                        annotated_framer = self.label_annotator.annotate(\n",
    "                            scene=annotated_framer,\n",
    "                            detections=all_detections,\n",
    "                            labels=labels_no_track\n",
    "                        )\n",
    "                    out.write(annotated_framer)\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        return tracked_data\n",
    "\n",
    "class VideoCrossMapper:\n",
    "    def __init__(self, object_detector: ObjectDetector):\n",
    "        self.object_detector = object_detector\n",
    "        self.sift = cv2.SIFT_create()\n",
    "        # FLANN parameters\n",
    "        FLANN_INDEX_KDTREE = 1\n",
    "        index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "        search_params = dict(checks=50)\n",
    "        self.flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "        self.consistent_object_mapping = {}\n",
    "        #config\n",
    "        self.MIN_MATCH_COUNT = 10  # number of good matches required to find a homography\n",
    "        self.SIFT_RATIO_TEST_THRESHOLD = 0.75 # (0.7-0.8 is standard)\n",
    "        self.RANSAC_REPROJECTION_THRESHOLD = 5.0 # maximum pixel distance for a point to be an inlier\n",
    "        self.MIN_IOU_THRESHOLD = 0.1\n",
    "\n",
    "        self.ellipse_annotator_final = sv.EllipseAnnotator(\n",
    "            color=sv.ColorPalette.from_hex([\"#000000\", \"#000000\", \"#2E2704\"]),\n",
    "            thickness=1\n",
    "        )\n",
    "        self.label_annotator_final = sv.LabelAnnotator(\n",
    "            color=sv.ColorPalette.from_hex(['#00BFFF', '#FF1493', '#FFD700']),\n",
    "            text_color=sv.Color.from_hex('#000000'),\n",
    "            text_position=sv.Position.TOP_CENTER,\n",
    "            text_thickness=0\n",
    "        )\n",
    "\n",
    "    def _get_frame(self, video_path: str, frame_number: int):\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_number)\n",
    "        ret, frame = cap.read()\n",
    "        cap.release()\n",
    "        if not ret:\n",
    "            print(f\"Warning: Could not read frame {frame_number} from {video_path}\")\n",
    "            return None\n",
    "        return frame\n",
    "\n",
    "    #transforms a single bounding box using homography\n",
    "    def _transform_bbox(self, bbox: List[float], H: np.ndarray) -> List[float]:\n",
    "        x1, y1, x2, y2 = bbox\n",
    "        corners = np.array([\n",
    "            [x1, y1],\n",
    "            [x2, y1],\n",
    "            [x2, y2],\n",
    "            [x1, y2]\n",
    "        ], dtype=np.float32).reshape(-1, 1, 2) # reshape for perspectiveTransform\n",
    "\n",
    "        # apply the homography\n",
    "        transformed_corners = cv2.perspectiveTransform(corners, H)\n",
    "\n",
    "        # calculating the new bounding box from the transformed corners\n",
    "        x_new = transformed_corners[:, 0, 0]\n",
    "        y_new = transformed_corners[:, 0, 1]\n",
    "\n",
    "        new_x1 = float(min(x_new))\n",
    "        new_y1 = float(min(y_new))\n",
    "        new_x2 = float(max(x_new))\n",
    "        new_y2 = float(max(y_new))\n",
    "\n",
    "        return [new_x1, new_y1, new_x2, new_y2]\n",
    "\n",
    "    def _calculate_iou(self, boxA: List[float], boxB: List[float]) -> float:\n",
    "        xA = max(boxA[0], boxB[0])\n",
    "        yA = max(boxA[1], boxB[1])\n",
    "        xB = min(boxA[2], boxB[2])\n",
    "        yB = min(boxA[3], boxB[3])\n",
    "\n",
    "        # compute the area of intersection rectangle\n",
    "        interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "        # compute the area of both the prediction and ground-truth rectangles\n",
    "        boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "        boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "        # edge case where boxes have zero area\n",
    "        if boxAArea <= 0 or boxBArea <= 0:\n",
    "            return 0.0\n",
    "\n",
    "        # compute the intersection over union\n",
    "        unionArea = float(boxAArea + boxBArea - interArea)\n",
    "        if unionArea <= 0:\n",
    "            return 0.0\n",
    "\n",
    "        iou = interArea / unionArea\n",
    "        return iou\n",
    "\n",
    "    def perform_cross_mapping(self, video1_path: str, video2_path: str,\n",
    "                              tracked_data_v1: TrackingDataType,\n",
    "                              tracked_data_v2: TrackingDataType) -> Dict:\n",
    "\n",
    "        frame_numbers_v1 = list(tracked_data_v1.keys()) if tracked_data_v1 else [0]\n",
    "        frame_numbers_v2 = list(tracked_data_v2.keys()) if tracked_data_v2 else [0]\n",
    "        max_frames = max(max(frame_numbers_v1), max(frame_numbers_v2)) + 1\n",
    "\n",
    "        for frame_num in range(max_frames):\n",
    "            print(f\"\\nProcessing Frame {frame_num}...\")\n",
    "\n",
    "            # Skip if no tracking data for this frame in both results\n",
    "            if frame_num not in tracked_data_v1 and frame_num not in tracked_data_v2:\n",
    "                print(f\"No tracking data for frame {frame_num}, skipping.\")\n",
    "                self.consistent_object_mapping[frame_num] = {}\n",
    "                continue\n",
    "\n",
    "            frame1 = self._get_frame(video1_path, frame_num)\n",
    "            frame2 = self._get_frame(video2_path, frame_num)\n",
    "\n",
    "            if frame1 is None or frame2 is None:\n",
    "                print(f\"Skipping frame {frame_num}\")\n",
    "                self.consistent_object_mapping[frame_num] = {}\n",
    "                continue\n",
    "\n",
    "            # Convert frames to grayscale for SIFT\n",
    "            gray1 = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
    "            gray2 = cv2.cvtColor(frame2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            #SIFT Feature Detection and Description\n",
    "            kp1, des1 = self.sift.detectAndCompute(gray1, None)\n",
    "            kp2, des2 = self.sift.detectAndCompute(gray2, None)\n",
    "\n",
    "            if des1 is None or des2 is None or len(kp1) < self.MIN_MATCH_COUNT or len(kp2) < self.MIN_MATCH_COUNT:\n",
    "                print(f\"Not enough SIFT features detected in frame {frame_num} for homography calculation.\")\n",
    "                self.consistent_object_mapping[frame_num] = {} # Store an empty entry\n",
    "                continue\n",
    "\n",
    "            # FLANN-based feature matching with ratio test\n",
    "            try:\n",
    "                matches = self.flann.knnMatch(des1, des2, k=2)\n",
    "            except cv2.error as e:\n",
    "                print(f\"Error during feature matching in frame {frame_num}: {e}\")\n",
    "                self.consistent_object_mapping[frame_num] = {}\n",
    "                continue\n",
    "\n",
    "            #Lowe's ratio test to filter good matches\n",
    "            good_matches = []\n",
    "            for match_pair in matches:\n",
    "                if len(match_pair) == 2:  # ensuring we have 2 matches\n",
    "                    m, n = match_pair\n",
    "                    if m.distance < self.SIFT_RATIO_TEST_THRESHOLD * n.distance:\n",
    "                        good_matches.append(m)\n",
    "\n",
    "            if len(good_matches) < self.MIN_MATCH_COUNT:\n",
    "                print(f\"Not enough good SIFT matches ({len(good_matches)}) in frame {frame_num} for homography calculation.\")\n",
    "                self.consistent_object_mapping[frame_num] = {} # Store an empty entry\n",
    "                continue\n",
    "\n",
    "            src_pts = np.float32([kp1[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "            dst_pts = np.float32([kp2[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "\n",
    "            # calculating homography matrix H using ransac\n",
    "            H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, self.RANSAC_REPROJECTION_THRESHOLD)\n",
    "\n",
    "            if H is None:\n",
    "                print(f\"Could not calculate homography for frame {frame_num}.\")\n",
    "                self.consistent_object_mapping[frame_num] = {} # Store an empty entry\n",
    "                continue\n",
    "\n",
    "            print(f\"Homography calculated for frame {frame_num} with {len(good_matches)} matches\")\n",
    "\n",
    "            current_frame_mapping = {}\n",
    "\n",
    "            v1_frame_data = tracked_data_v1.get(frame_num, {})\n",
    "            v2_frame_data = tracked_data_v2.get(frame_num, {})\n",
    "\n",
    "            for class_id_v1, tracking_ids_v1 in v1_frame_data.items():\n",
    "                if class_id_v1 not in current_frame_mapping:\n",
    "                    current_frame_mapping[class_id_v1] = {}\n",
    "\n",
    "                for tracking_id_v1, bbox_data_v1 in tracking_ids_v1.items():\n",
    "                    if isinstance(bbox_data_v1, list):\n",
    "                        if len(bbox_data_v1) == 4 and all(isinstance(x, (int, float)) for x in bbox_data_v1):\n",
    "                            # bbox format [x1,y1,x2,y2]\n",
    "                            bbox_v1 = bbox_data_v1\n",
    "                        else:\n",
    "                            print(f\"Invalid bbox format for tracking_id {tracking_id_v1}\")\n",
    "                            continue\n",
    "                    else:\n",
    "                        print(f\"error for tracking_id {tracking_id_v1}\")\n",
    "                        continue\n",
    "\n",
    "                    try:\n",
    "                        transformed_bbox_v2 = self._transform_bbox(bbox_v1, H)\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error transforming bbox: {e}\")\n",
    "                        continue\n",
    "\n",
    "                    best_iou = self.MIN_IOU_THRESHOLD\n",
    "                    best_match_info = None\n",
    "\n",
    "                    for class_id_v2, tracking_ids_v2 in v2_frame_data.items():\n",
    "                        #matching only same class_ids\n",
    "                        if class_id_v1 != class_id_v2:\n",
    "                            continue\n",
    "\n",
    "                        for tracking_id_v2, bbox_data_v2 in tracking_ids_v2.items():\n",
    "                            # Handle different bbox formats for video 2\n",
    "                            if isinstance(bbox_data_v2, list):\n",
    "                                if len(bbox_data_v2) == 4 and all(isinstance(x, (int, float)) for x in bbox_data_v2):\n",
    "                                    bbox_v2_detected = bbox_data_v2\n",
    "                                else:\n",
    "                                    continue\n",
    "                            else:\n",
    "                                continue\n",
    "\n",
    "                            iou = self._calculate_iou(transformed_bbox_v2, bbox_v2_detected)\n",
    "\n",
    "                            if iou > best_iou:\n",
    "                                best_iou = iou\n",
    "                                best_match_info = {\n",
    "                                    'class_id': class_id_v2,\n",
    "                                    'tracking_id': tracking_id_v2,\n",
    "                                    'bbox': bbox_v2_detected,\n",
    "                                    'iou': iou\n",
    "                                }\n",
    "\n",
    "                    # updating the data structure\n",
    "                    object_mapping_entry = {\n",
    "                        'video1_bbox': bbox_v1,\n",
    "                        'video2_transformed_bbox': transformed_bbox_v2,\n",
    "                        'video2_matched_detection': best_match_info # will be None if no good match\n",
    "                    }\n",
    "\n",
    "                    current_frame_mapping[class_id_v1][tracking_id_v1] = object_mapping_entry\n",
    "\n",
    "            self.consistent_object_mapping[frame_num] = current_frame_mapping\n",
    "\n",
    "        print(f\"Cross Mapping Successful!\")\n",
    "        return self.consistent_object_mapping\n",
    "\n",
    "    def generate_consistent_id_mapping(self, consistent_object_mapping: Dict):\n",
    "        tracker_mapping_counts = {}  # {(class_id, track_id_v1, track_id_v2): count}\n",
    "        for frame_num, frame_data in consistent_object_mapping.items():\n",
    "            for class_id, tracks in frame_data.items():\n",
    "                for track_id_v1, mapping in tracks.items():\n",
    "                    match_info = mapping.get('video2_matched_detection')\n",
    "                    if match_info is not None:\n",
    "                        track_id_v2 = match_info['tracking_id']\n",
    "                        key = (class_id, track_id_v1, track_id_v2)\n",
    "                        tracker_mapping_counts[key] = tracker_mapping_counts.get(key, 0) + 1\n",
    "\n",
    "        sorted_mappings = sorted(tracker_mapping_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        # This dictionary will map a tacticam_id to its most likely broadcast_id.\n",
    "        tacticam_to_broadcast_id_map = {}\n",
    "\n",
    "        broadcast_ids_mapped = set()\n",
    "        tacticam_ids_mapped = set()\n",
    "\n",
    "        for (class_id, b_id, t_id), count in sorted_mappings:\n",
    "            if b_id not in broadcast_ids_mapped and t_id not in tacticam_ids_mapped:\n",
    "                tacticam_to_broadcast_id_map[t_id] = b_id\n",
    "                broadcast_ids_mapped.add(b_id)\n",
    "                tacticam_ids_mapped.add(t_id)\n",
    "\n",
    "        print(\"Created a stable global mapping for consistent IDs.\")\n",
    "        print(f\"Total stable mappings created: {len(tacticam_to_broadcast_id_map)}\")\n",
    "\n",
    "        simplified_mappings = {}\n",
    "        for frame_num, frame_data in consistent_object_mapping.items():\n",
    "            simplified_mappings[frame_num] = {}\n",
    "            for class_id, tracks in frame_data.items():\n",
    "                for track_id_v1, mapping in tracks.items():\n",
    "                    match_info = mapping.get('video2_matched_detection')\n",
    "                    if match_info is not None:\n",
    "                        if class_id not in simplified_mappings[frame_num]:\n",
    "                            simplified_mappings[frame_num][class_id] = {}\n",
    "                        simplified_mappings[frame_num][class_id][track_id_v1] = {\n",
    "                            'tacticam_id': match_info['tracking_id'],\n",
    "                            'iou': match_info['iou']\n",
    "                        }\n",
    "\n",
    "        MAPPINGS_FILE = 'tracker_id_mappings.json'\n",
    "        with open(MAPPINGS_FILE, 'w') as f:\n",
    "            json.dump(simplified_mappings, f, indent=2)\n",
    "        print(\"\\nTracker mappings saved to 'tracker_id_mappings.json'\")\n",
    "\n",
    "        return tacticam_to_broadcast_id_map\n",
    "\n",
    "    def annotate_with_consistent_ids(self, input_video_path: str,\n",
    "                                     output_video_name: str,\n",
    "                                     tracked_data: TrackingDataType,\n",
    "                                     is_broadcast_video: bool,\n",
    "                                     tacticam_to_broadcast_id_map: Dict):\n",
    "\n",
    "        output_path = f\"results/{output_video_name}\"\n",
    "        print(f\"\\n Starting consistent annotation for: {input_video_path}\")\n",
    "\n",
    "        cap = cv2.VideoCapture(input_video_path)\n",
    "        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "        frame_num = 0\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            frame_num += 1\n",
    "\n",
    "            detections_in_frame = tracked_data.get(frame_num, {})\n",
    "\n",
    "            if not detections_in_frame:\n",
    "                out.write(frame) # original frame if no detections\n",
    "                continue\n",
    "\n",
    "            all_bboxes = []\n",
    "            all_class_ids = []\n",
    "            all_tracker_ids = []\n",
    "\n",
    "            for class_id, tracks in detections_in_frame.items():\n",
    "                for tracker_id, bbox in tracks.items():\n",
    "                    all_bboxes.append(bbox)\n",
    "                    all_class_ids.append(class_id)\n",
    "                    all_tracker_ids.append(tracker_id)\n",
    "\n",
    "            # Convert to supervision Detections object\n",
    "            sv_detections = sv.Detections(\n",
    "                xyxy=np.array(all_bboxes),\n",
    "                class_id=np.array(all_class_ids),\n",
    "                tracker_id=np.array(all_tracker_ids)\n",
    "            )\n",
    "\n",
    "            # Generate consistent labels\n",
    "            labels = []\n",
    "            for tracker_id, class_id in zip(sv_detections.tracker_id, sv_detections.class_id):\n",
    "                class_name = self.object_detector.model.names[class_id][0].upper()\n",
    "\n",
    "                if is_broadcast_video:\n",
    "                    #  broadcast ID is the master ID\n",
    "                    final_id = tracker_id\n",
    "                else:\n",
    "                    #its corresponding master (broadcast) ID\n",
    "                    final_id = tacticam_to_broadcast_id_map.get(tracker_id, f\"T{tracker_id}\")\n",
    "\n",
    "                labels.append(f\"#{final_id} {class_name}\")\n",
    "\n",
    "            # Annotate the frame\n",
    "            annotated_frame = frame.copy()\n",
    "            annotated_frame = self.ellipse_annotator_final.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=sv_detections\n",
    "            )\n",
    "            annotated_frame = self.label_annotator_final.annotate(\n",
    "                scene=annotated_frame,\n",
    "                detections=sv_detections,\n",
    "                labels=labels\n",
    "            )\n",
    "\n",
    "            out.write(annotated_frame)\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        print(f\"Consistently annotated video to: {output_path}\")\n",
    "\n",
    "    def combine_videos(self, video1_path: str, video2_path: str, output_path: str):\n",
    "        broadcast_cap = cv2.VideoCapture(video1_path)\n",
    "        tacticam_cap = cv2.VideoCapture(video2_path)\n",
    "\n",
    "        width = int(broadcast_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "        height = int(broadcast_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "        fps = broadcast_cap.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "        # output width will be double the width\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(output_path, fourcc, fps, (width * 2, height))\n",
    "\n",
    "        while True:\n",
    "            ret1, frame1 = broadcast_cap.read()\n",
    "            ret2, frame2 = tacticam_cap.read()\n",
    "\n",
    "            if not ret1 or not ret2:\n",
    "                break\n",
    "            combined_frame = np.hstack((frame1, frame2))\n",
    "            out.write(combined_frame)\n",
    "\n",
    "        broadcast_cap.release()\n",
    "        tacticam_cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        print(\"Your videos have been consistently mapped. Your Output is Ready.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbfb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define video paths\n",
    "broadcast_video_path = 'br-frame_matched.mp4'\n",
    "tacticam_video_path = 'ta-frame_matched.mp4'\n",
    "\n",
    "# Step 1: Object Detection\n",
    "detector = ObjectDetector(model_path='best.pt')\n",
    "print(f\"Processing broadcast video: {broadcast_video_path}\")\n",
    "detector.detect_and_annotate(broadcast_video_path, \"Broadcast\")\n",
    "print(f\"Processing tacticam video: {tacticam_video_path}\")\n",
    "detector.detect_and_annotate(tacticam_video_path, \"Tacticam\")\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\nAll video processing tasks completed.\")\n",
    "\n",
    "# Step 2: Object Tracking\n",
    "tracker = ObjectTracker(model=detector.model)\n",
    "broadcast_tracked_players = tracker.track_and_annotate(broadcast_video_path, \"Broadcast\")\n",
    "tacticam_tracked_players = tracker.track_and_annotate(tacticam_video_path, \"Tacticam\")\n",
    "print(\"Video processing complete. Output saved.\")\n",
    "\n",
    "# Step 3: Video Cross Mapping and Consistent Annotation\n",
    "mapper = VideoCrossMapper(object_detector=detector)\n",
    "consistent_object_mapping = mapper.perform_cross_mapping(\n",
    "    broadcast_video_path, tacticam_video_path,\n",
    "    broadcast_tracked_players, tacticam_tracked_players\n",
    ")\n",
    "\n",
    "tacticam_to_broadcast_id_map = mapper.generate_consistent_id_mapping(consistent_object_mapping)\n",
    "\n",
    "mapper.annotate_with_consistent_ids(\n",
    "    input_video_path=broadcast_video_path,\n",
    "    output_video_name=\"Final-Consistent-Broadcast.mp4\",\n",
    "    tracked_data=broadcast_tracked_players,\n",
    "    is_broadcast_video=True,\n",
    "    tacticam_to_broadcast_id_map=tacticam_to_broadcast_id_map\n",
    ")\n",
    "\n",
    "mapper.annotate_with_consistent_ids(\n",
    "    input_video_path=tacticam_video_path,\n",
    "    output_video_name=\"Final-Consistent-Tacticam.mp4\",\n",
    "    tracked_data=tacticam_tracked_players,\n",
    "    is_broadcast_video=False,\n",
    "    tacticam_to_broadcast_id_map=tacticam_to_broadcast_id_map\n",
    ")\n",
    "cv2.destroyAllWindows()\n",
    "print(\"\\n All videos have been generated again with consistent tracking IDs.\")\n",
    "\n",
    "# Step 4: Combine Final Videos\n",
    "mapper.combine_videos(\n",
    "    \"results/Final-Consistent-Broadcast.mp4\",\n",
    "    \"results/Final-Consistent-Tacticam.mp4\",\n",
    "    \"results/Final_Result.mp4\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
